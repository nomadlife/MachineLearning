0:00
때로 사람들은 지원 벡터 머신에 대해 이야기합니다.이 비디오에서는이 비디오에서 큰 의미의 분류 기준으로 무엇을 의미하는지 알려주고 있으며 이는 SVM 가설이 어떻게 생겼는지에 대한 유용한 그림을 제공합니다. 지원 벡터 머신에 대한 비용 함수는 다음과 같습니다.
0:21
왼쪽의 여기는 z1 함수에 대한 비용 1을 플롯했습니다.
0:30
'Z'함수의 제로, 여기서 수평 축에 'Z'가 있습니다. 이제 이러한 비용 함수를 작게 만드는 데 필요한 것이 무엇인지 생각해 봅시다.
0:39
긍정적 인 예가있는 경우 y가 1이면 Z가 1보다 크거나 같을 때만 Z의 비용 1이 0입니다. 즉, 긍정적 인 예가 있다면 우리는 정말로 theta transpose x가 1보다 크거나 같고, 반대로 y가 0 인 경우 z 함수의이 비용 제로를보고,
1:01
그렇다면 z가 1보다 작은이 영역에서만 z가 0과 동일하므로 비용이 0이됩니다. 이것은 지원 벡터 머신의 흥미로운 속성입니다. 그래서 y가 1과 같다면 우리가 정말로 필요로하는 것은 theta transpose x가 0보다 큰 것입니다.
1:22
그리고 이것은 우리가 정확하게 분류하는 것을 의미 할 것입니다. 왜냐하면 theta transpose x가 0보다 크면 우리의 가설은 0을 예측할 것이기 때문입니다. 그리고 마찬가지로, 네가 부정적인 예제를 가지고 있다면, 실제로 원하는 것은 theta transpose x가 0보다 작고 그 것이 올바른 예제인지를 확인하는 것입니다. 그러나 지원 벡터 머신은 그 이상을 원합니다. 그것은, 알다시피, 간신히 모범을 보일뿐입니다. 그렇다면 단지 0보다 조금 더 크게하지 마십시오. 내가 정말로 원하는 것은 이것이 제로보다 훨씬 더 크다고 할지도 모릅니다. 아마 1보다 크거나 같을 것입니다. 저는 이것을 0보다 훨씬 작게하고 싶습니다. 아마 -1보다 작거나 같을 수도 있습니다. 따라서 이것은 추가 안전 계수 또는 안전 여유 계수를 지원 벡터 머신에 구축합니다. 로지스틱 회귀 (logistic regression)도 물론 유사하지만 지원 벡터 머신의 컨텍스트에서 어떤 결과가 발생하는지 보거나 그 결과가 무엇인지 확인해 봅시다.
2:14
구체적으로, 내가하고 싶은 것은이 상수 C를 매우 큰 값으로 설정 한 사례를 생각해 보겠습니다. 그래서 C를 매우 큰 값으로 설정한다고 가정 해 보겠습니다. 수십만의 거대한 숫자가있을 수 있습니다.
2:29
지원 벡터 머신이 무엇을하는지 봅시다. C가 매우 크다면, 최소화 할 때
2:36
이 최적화 목표는 값을 선택하라는 동기가 높기 때문에 첫 번째 항은 0과 같습니다.
2:44
따라서 최적화 문제를 맥락에서 이해하려고 시도해 봅시다. 목표의 첫 번째 용어를 0으로 만드는 것은 무엇을 의미할까요? 아마도 C를 거대한 상수로 설정할 것이고, 이것이 희망이 될 것입니다. 지원 벡터 머신이 어떤 종류의 가설을 배울 지에 대한 추가적인 직감을 제공해야합니다. 그래서 우리는 당신이 첫 번째 용어를 0으로 만들고 싶다면 y = 1이라는 레이블을 가진 훈련 예제가있을 때마다, 필요한 것은 theta transpose xi가 더 크거나 같도록 theta의 값을 찾는 것입니다 그리고 마찬가지로, 레이블 0 인 예제가있을 때 Z의 비용 비용 0을 확실히하기 위해 비용이 0인지 확인하기 위해 theta transpose xi가 0보다 작거나
3:37
-1과 같습니다. 따라서 최적화 문제를 지금과 같이 생각하고 실제로 매개 변수를 선택하고이 첫 번째 항이 0과 같음을 보여 주면 남은 것은 다음과 같은 최적화 문제입니다. 우리는 첫 번째 항을 0으로 최소화 할 것이므로 C를 0으로 만듭니다. 왜냐하면 우리는 매개 변수를 선택하여 0과 1을 더한 다음 두 번째 항을 알고이 첫 번째 항이 'C' 그래서 제로가 될 것이라는 것을 알기 때문에 이것을 건너 뛰자. 그리고 이것은 theta transpose x (i)가 다음과 같거나 더 크다는 제약에 종속 될 것입니다.
4:18
하나, y (i)
4:22
네가 음의 예를 가지고있을 때마다 theta transpose x (i)는 -1보다 작거나 같습니다.이 최적화 문제를 풀 때 매개 변수 theta의 함수로 최소화 할 때
4:40
당신은 매우 흥미로운 결정 경계를 얻습니다. 구체적으로, 이와 같은 데이터 세트를 양수 및 음수 예제로 보면이 데이터
4:50
선형 적으로 분리가 가능하다는 것을 의미합니다. 즉, 직선이 존재한다는 것을 의미합니다. 다른 직선이 많이 있지만, 양수와 음수의 예제를 완벽하게 구분할 수 있습니다. 예를 들어 여기에는 하나의 의사 결정 경계가 있습니다.
5:04
긍정적이고 부정적인 예를 구분하지만, 어쨌든 그것은 매우 자연스러운 것과 같지 않습니까? 또는 더 나쁜 그림을 그려 보면 긍정적이고 부정적인 예를 구분하지만 결정적으로는 간과 할 수있는 또 다른 결정 경계가 있습니다. 그러나 그 중 어느 것도 특히 좋은 선택 같지는 않습니다.
5:20
Support Vector Machines는이 결정 경계를 대신 선택합니다.이 경계는 검정으로 그립니다.
5:29
그리고 그것은 내가 마젠타 색이나 초록색으로 그린 ​​것보다 훨씬 더 나은 결정 경계처럼 보입니다. 검은 색 선은보다 강건한 구분선처럼 보이며, 양수 및 음수 예제를 분리하는 더 나은 작업을 수행합니다. 그리고 수학적으로, 그것이하는 것은,이 검은 결정 경계는 더 큰 거리를 가지고 있습니다.
5:49
그 거리를 마진이라고 부릅니다.이 두 개의 여분의 파란색 선을 그릴 때 검은 색 결정 경계선이 내 훈련 예와 약간의 최소 거리를 가지고있는 반면 마젠타 색 선과 녹색 선은 훈련에 매우 가깝습니다. 예.
6:04
그리고 나서 그것은 흑인 선보다 긍정적이고 부정적 인 구분을 덜 잘하는 것처럼 보입니다. 그래서이 거리는 지원 벡터 머신의 마진이라 불리우며 SVM은 가능한 한 큰 여백으로 데이터를 분리하려고하기 때문에 SVM에 일정한 견고성을 부여합니다.
6:29
따라서 지원 벡터 머신을 대용량 분류기라고도하며 이는 실제로 이전 슬라이드에서 적어 놓은 최적화 문제의 결과입니다. 이전 슬라이드에서 내가 쓴 최적화 문제가 어떻게이 큰 마진 분류 자로 이어지는 지 궁금해 할 것입니다.
6:48
나는 아직 그것을 설명하지 않았다는 것을 알고있다. 그리고 다음 비디오에서는 왜 최적화 문제가 우리에게이 큰 마진 분류자를주는 지에 대한 직감에 대해 약간 개략적으로 설명 할 것입니다. 그러나 이것은 SVM이 선택할 가설의 종류가 무엇인지 이해하려고 시도 할 때 명심해야 할 유용한 기능입니다. 즉, 가능한 한 큰 여백으로 양수 및 음수 예제를 분리하려고합니다.

(QUIZ)

7:12
저는이 직관에서 큰 마진 분류 자에 관해 마지막으로 한 마디를 말하고 싶습니다. 그래서 우리는 그 정규화 개념 인 C가 매우 큰 경우에이 큰 마진 분류 설정을 썼습니다. 나는 그것을 수십만 가지로 설정했다고 생각합니다. . 이와 같이 데이터 세트가 주어지면 양수와 음수 예제를 큰 여백으로 구분하는 결정 경계를 선택할 수 있습니다.
7:37
이제 SVM은이 큰 마진보기가 제안 할 수있는 것보다 실제로 sligthly 더 정교합니다. 특히, 학습 알고리즘이 아웃 라이어에 민감 할 수 있으므로 큰 마진 분류자를 사용하는 경우 화면에 표시된 것과 같은 특별한 긍정적 인 예를 추가하면됩니다. 그가 한 가지 예를 든다면 큰 마진으로 데이터를 분리하는 것처럼 보입니다.
8:02
어쩌면 내가 그런 결정의 경계를 알게 될거야, 그렇지? 그것은 마젠타 색 선이고 단일 예제를 기반으로 한 단일 이상 값을 기반으로하는 것은 사실 명확하지 않으며 내 결정 경계를 검정색 경계에서 마젠타 색 경계로 변경하는 것이 실제로 실제로 바람직하지 않음이 분명합니다.
8:20
따라서 C가 정규화 매개 변수 C가 매우 큰 경우 SVM이 실제로 수행 할 작업이며, 검정에서 자홍색으로 결정 경계를 변경하지만 C를 사용하는 경우 C가 비교적 작 으면, 너무 크지 않으면이 검은 결정 경계로 끝납니다. 물론 데이터가 선형으로 분리되지 않아 여기에 긍정적 인 예가 있거나 여기에 몇 가지 부정적인 예가있는 경우 SVM이 올바른 작업을 수행합니다. 그래서 정말 큰 마진 등급 분류 자의 그림은 규제 매개 변수 C가 매우 큰 경우에만 더 나은 직관을 제공하는 그림입니다. C가 람다를 능가하는 것과 비슷한 역할을합니다. 여기서 Lambda는 이전에 가지고 있던 정규화 매개 변수입니다. 그래서 Lambda가 매우 작거나 Lambda가 매우 작아서이 Magenta 결정 경계와 같은 것으로 끝나면 Lambda는 매우 크거나 동등합니다.
9:28
실제로 지원 벡터 머신을 적용 할 때 C가 그다지 크지 않을 때,
9:34
여기와 같은 몇몇 이상 치를 무시하면 더 나은 일을 할 수 있습니다. 또한 데이터가 선형으로 분리되지 않는 경우에도 잘 수행하고 합리적인 작업을 수행하십시오. 그러나 약간의 시간이 걸릴 지원 벡터 머신의 맥락에서 편견과 분산에 대해 이야기 할 때, 정규화 매개 변수를 포함하는 모든 절충 사항이 그 때 더 명확해질 수 있기를 바랍니다. 따라서이 지원 벡터 머신이 큰 마진으로 데이터를 분리하려고하는 큰 마진 분류기로서 어떻게 기능하는지에 대한 직관을 제공하기를 바랍니다. 기술적으로이 뷰의 그림은 매개 변수 C가 매우 큰 경우에만 유효합니다.
10:10
지원 벡터 머신에 대해 생각할 수있는 유용한 방법입니다.
10:13
이 동영상에는 한 가지 빠진 단계가 있습니다. 슬라이드에 적어 놓은 최적화 문제가 왜 큰 마진 등급 분류 자로 이어지는 지, 다음 동영상에서이 동영상에서는 어떻게하지 않았는가? 나는 우리가 썼던 최적화 문제가 어떻게 큰 마진 분류자를 생성하는지에 대한 개별적인 추론을 설명하기 위해 그 뒤에있는 수학에 대해 좀 더 자세히 설명 할 것이다.
