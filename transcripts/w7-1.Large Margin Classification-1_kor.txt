0:00
지금까지 다양한 차이 학습 알고리즘을 보았습니다. 감독 학습을 사용하면 많은 감독 학습 알고리즘의 성능이 매우 유사 할 것입니다. 학습 알고리즘 a 또는 학습 알고리즘 b를 사용하는지 여부는 중요하지 않지만 중요한 것은 종종 이러한 학습 데이터의 양과 같은 것입니다 알고리즘을 적용하고 이러한 알고리즘을 적용하는 데 필요한 기술을 습득 할 수 있습니다. 학습 알고리즘에 제공하기 위해 디자인 한 기능을 선택하는 것과 컬러화 매개 변수를 선택하는 방법 등이 있습니다. 그러나 매우 강력하고 업계 및 학계에서 널리 사용되는 알고리즘이 하나 더 있습니다. 지원 알고리즘은 지원 벡터 시스템이라고합니다. 또한 로지스틱 회귀 분석과 신경망에 비해 SVM (Support Vector Machine)은 때로는 더 깨끗하고 때로는 더 강력한 복잡한 비선형 함수를 학습하는 방법을 제공합니다. 그리고 다음 비디오를 통해 그 이야기를 나누겠습니다. 이 과정의 후반부에 나는 매우 간 단히 묘사 된 것과 같이 다양한 감독 알고리즘의 빠른 조사를 할 것이다. 그러나 지원 벡터 머신은 그 인기와 강력 함을 감안할 때, 우리가 개발 한 다른 학습 알고리즘처럼이 과정에서 상당한 시간을 할애 할 수있는 감독 알고리즘의 마지막 일 것입니다. 최적화 목표에 대해 이야기함으로써 시작하십시오. 자,이 알고리즘을 시작해 봅시다.
1:29
지원 벡터 머신을 설명하기 위해 실제로는 로지스틱 회귀부터 시작하여 어떻게 조금 수정하고 지원 벡터 머신을 얻는 지 보여줍니다. 따라서 로지스틱 회귀 분석에서 우리는 친숙한 형태의 가설과 오른쪽에 보이는 시그 모이 드 활성화 함수를가집니다.
1:50
그리고 수학의 일부를 설명하기 위해, 저는 z를 사용하여 theta transpose의 공리를 나타낼 것입니다.
1:57
이제 우리가 로지스틱 회귀를 어떻게 할 것인지 생각해 봅시다. 우리가 y = 1 인 예제가 있고 이것에 의해 트레이닝 세트 나 테스트 세트 또는 교차 검증 세트의 예제를 의미하지만, y가 1과 같을 때 우리는 x의 h 하나에 가까울 것이다. 맞아, 우리는이 예제를 정확하게 분류하기를 희망하고있다. 그리고 h of x 가 1에 가까운 것은 theta transpose x가 0보다 커야한다는 것을 의미합니다. 따라서 0보다 훨씬 더 큰 의미의 부호보다 더 큰 의미가 있습니다. 왜냐하면 그것은 z이므로 transpose의 theta입니다. x는 z가 0보다 훨씬 큰 경우입니다. 물류 진보의 결과가 하나에 가까워짐.
2:44
반대로, y가 0 인 예제가 있다면, 우리가 기대하는 것은 가설이 0에 가까운 값을 출력한다는 것입니다. 그리고 그것은 z 값이 0보다 훨씬 작은 theta transpose x에 해당합니다. 이는 값을 0에 가깝게한다는 가설에 해당하기 때문입니다.
3:02
로지스틱 회귀의 비용 함수를 살펴보면 각 예제 (x, y)가 전체 비용 함수에 이와 같은 용어를 제공한다는 것을 알 수 있습니다.
3:14
따라서 전체 비용 함수에 대해 모든 연쇄 사례와 1 개월 이상에 대한 합계를 구할 것입니다.이 표현은 여기에 하나의 교육 사례가 전체 목적 함수에 기여하는 용어이므로 여기에 서둘러 설명 할 수 있습니다. .
3:32
이제 제가 가설의 가을에 대한 정의를 가져 와서 여기에 연결하면, 내가 얻은 것은 각 교육 사례가 M 이상을 무시하면서이 용어를 기여하지만 회귀 회귀에 대한 전체 비용 함수에 해당 용어를 기여한다는 것입니다 .
3:51
이제 y가 1과 같고 y가 0 일 때의 두 가지 경우를 생각해 봅시다. 첫 번째 경우에 y가 1과 같다고 가정 해 봅시다.이 경우 목적의 첫 번째 용어 만 중요합니다. y가 1과 같은 경우이 1 빼기 용어는 0과 같기 때문입니다.
4:13
따라서 y가 1과 같을 때, 예제 x comma y에서 y가 1 일 때 우리가 얻는 것은이 용어입니다. 하나 이상의 마이너스 로그, E를 음수 Z에 더한 마지막 줄 I 'Z를 사용하여 데이터를 옮겨 놓은 X와 물론 비용을 나타냅니다. Y가 1과 같으면이 마이너스 선을 가져야합니다. 따라서이 값은 1과 같습니다. 제가 작성한 표현에서 어떤 방식으로도 단순화합니다. 이리.
4:41
이 함수를 z의 함수로 그려 보면, 슬라이드의 왼쪽 아래에이 곡선이 표시됩니다. 그래서 z가 크면, 즉 theta transpose x가 클 때 z 값이 상당히 작아서 소비에 매우 작은 기여를한다는 것을 알 수 있습니다. 그리고 이것은 로지스틱 회귀가 y = 1 인 긍정적 인 예를 볼 때 왜 십자가 함수에서이 용어가 작기 때문에 theta transport x를 매우 크게 설정하려고하는지 설명합니다. 지원되는 vec 시스템을 채우기 위해 여기에 우리가 할 일이 있습니다. 우리는이 십자가 함수를 취합니다.이 빼기 로그 1에 1을 더한 다음 e를 음수 값으로 바꾸고 조금 수정하십시오. 이 지점 1을 여기로 가져와 사용하게 될 교차 함수를 그려 보겠습니다. 새로운 패스 함수는 여기서부터 평평해질 수 있습니다. 그런 다음 로지스틱 회귀와 마찬가지로 직선으로 성장하는 무언가를 그립니다. 그러나 이것은이 부분에서 직선이 될 것입니다. 그래서 저는 마젠타 색으로 그려진 커브와, 단지 로지스틱 회귀에 의해 사용 된 십자가 함수에 꽤 가까운 근사치 인 경우에 저는 보라색과 마젠타를 그린 것입니다. 단 두 개의 선분으로 구성되어 있다는 점을 제외하면 오른쪽에 평평한 부분이 있고 왼쪽에는이 직선 부분이 있습니다. 그리고 직선 부분의 기울기에 대해 너무 걱정하지 마십시오. 그다지 중요하지 않습니다. 그러나 이것은 y가 1과 같을 때 사용할 새로운 비용 함수이며, 로지스틱 회귀와 매우 유사한 것을해야한다고 상상할 수 있습니다. 그러나 이것은 지원 벡터 머신에 계산상의 이점을 제공 할 것이고 나중에 더 쉬운 최적화 문제를 줄 것이라고 밝혀졌습니다
6:38
소프트웨어가 쉽게 해결할 수 있습니다. 방금 y가 1 인 경우에 대해 이야기했습니다. 다른 경우는 y가 0 인 경우입니다. 이 경우 비용을 살펴보면 첫 번째 용어가 사라지기 때문에 두 번째 용어 만 적용됩니다. 맞습니까? y가 0이면 여기에 0이 있으므로 위 식의 두 번째 항만 남습니다. 그래서 예제의 비용, 또는 비용 함수의 기여는 여기에이 용어로 주어질 것입니다. 그리고 그것을 z의 함수로 플롯하면 수평축에 순수한 z를 갖게됩니다. 그리고 지원 벡터 머신의 경우,이 파란색 선을 비슷한 것으로 대체하면서 동시에 새로운 비용으로 대체 할 것입니다. 여기서이 평면은 여기서 0입니다. 그리고 나서 그것은 마치 직선처럼 자랍니다. 그래서이 두 함수 이름을 알려줍니다. 왼쪽에있는이 함수는 z의 cost subscript 1을 호출 할 것이고,이 함수는 z의 cost subscript 0을 호출 할 것입니다. 그리고 첨자는 단지 y가 0 일 때와 비교할 때 y가 1 일 때의 비용을 나타냅니다. 이러한 정의로 무장 한 이제 우리는 지원 벡터 머신을 만들 준비가되었습니다.
우리가 로지스틱 회귀를 위해 가지는 세타의 비용 함수가 있습니다. 이 방정식이 익숙하지 않은 것처럼 보이는 경우 이전에 우리가 빼기 기호를 외부에 가지고 있었기 때문에였습니다.하지만 여기서는 대신 이러한 빼기 기호 내에서 빼기 기호를 이동했기 때문에 조금 다르게 보입니다.
8:12
우리가해야 할 지원 벡터 머신은 기본적으로 이것을 취하여 이것을 z의 cost1, 즉 theta transpose x의 cost1로 대체합니다. 그리고 우리는 이것을 취하여 z의 cost0, 즉 theta transpose x의 cost0로 대체 할 것입니다. 비용 중 하나의 기능은 이전 슬라이드에서 본 것과 같습니다. 그리고 비용 제로 기능, 다시 이전 슬라이드에서 우리가 가진 것, 그리고 그것은 다음과 같습니다. 그래서 우리가 지원 벡터 머신에 대해 가지고있는 것은 M 이상의 하나의 최소화 문제입니다. Y I times 비용의 합계, theta transpose X I, 그리고 Y I times는 theta transpose X I의 제로를 일으킨 다음 보통의 정규화 매개 변수를 더한 것입니다.
9:21
그렇게. 관습에 따라 벡터 머신의 지원을 위해 실제로 약간 다른 것을 작성합니다. 우리는 이것을 아주 약간 다르게 다시 매개 변수화합니다.
9시 33 분
첫째, 우리는 1 개월 이상의 용어를 제거 할 것입니다. 그리고 이것은 사람들이 지원 벡터 머신에 사용하는 약간 다른 관례 일뿐입니다. 그러나 여기에 내가 의미하는 바가있다. 당신은 이것을하는 한 가지 방법입니다, 우리는 m 용어를 통해 이러한 것을 제거 할 것입니다. 그러면 베타 권리와 똑같은 최적의 가치를 얻을 수 있습니까? 왜냐하면 내가 m 이상의 하나는 상수 일 뿐이므로 나는이 최소화 문제를 앞에있는 n 개 이상으로 해결했는지 아닌지. 나는 세타에 대한 동일한 최적의 값으로 끝나야한다. 제가 의미하는 바는 여기에 예제를 드리 자면, 최소화 문제가 있다고 가정 해 봅시다. U의 5 분의 2에 1을 더한 긴 숫자의 U를 최소화합니다. 최소 U는 5가됩니다.
10:23
이제이 목적 함수를 취해서 10을 곱하면된다. 그래서 여기에서 나의 최소화 문제는 U, 10 U에서 5 제곱에 10을 더한 값이다.이 값을 최소화하는 U의 값은 여전히 U와 같다. 그래서 여러분이 극소화하고있는 어떤 것을 곱하면됩니다.이 경우에 상수 10을 곱하면, 우리에게주는 U의 값은 변하지 않습니다.이 값은이 함수를 최소화합니다. 그래서 같은 방식으로, 제가 한 것은 M을 건너는 것입니다. 제가하고있는 모든 것은 객관적인 함수에 상수 M을 곱해서 쎄타의 값을 바꾸지 않습니다. 그것은 최소한을 달성합니다. 로지스틱 회귀 대신 SVM을 사용할 때 표준 표기법 인 표기 변경의 두 번째 비트는 다음과 같습니다. 따라서 로지스틱 회귀 (logistic regression)의 경우 목적 함수에 두 개의 항을 추가합니다. 첫 번째는이 용어이며, 훈련 세트에서 오는 비용이며 두 번째는 정규화 용어 인이 행입니다.
11:27
우리가 가지고있는 것은 우리가 가지고있는 것입니다. 우리는 A plus, 그리고 나서 나의 regularization parameter 람다라고 말함으로써 이들 사이의 절충을 통제합니다. 그리고 나서 다른 용어 B를 곱합니다. 여기서 A는이 첫 번째 용어를 나타 내기 위해 사용하고 B는 람다없이 두 번째 용어를 나타냅니다.
11:51
그리고 이것을 A plus lambda B로 우선 순위를 매기는 대신에,이 정규화 매개 변수 lambda에 대해 다른 값을 설정하는 것이 었습니다. 우리는 훈련을 얼마나 원했는지, 즉 A를 최소화하고, 우리가 매개 변수의 값을 작게 유지하는 데 얼마나 신경을 쓰는지, 매개 변수는 지원 벡터 시스템에 대한 B인지, 관습에 따라 다른 매개 변수를 사용할지 여부를 결정합니다. 그래서 여기에서 람다를 사용하여 첫 번째와 두 번째 용어 사이의 상대적 대기를 제어합니다. 우리는 관례에 따라 C 라 불리는 다른 매개 변수를 사용하려고하고 C 시간을 a + B로 최소화하도록 설정합니다. 따라서 로지스틱 회귀에 대해 매우 큰 값의 람다를 설정하면 B를 매우 많이 줄 것입니다. 높은 체중. 여기에 우리가 C를 매우 작은 값으로 설정하면 A보다 B에 C보다 훨씬 더 빠른 속도로 응답합니다. 따라서이 방법은 거래를 제어하는 ​​다른 방법 일 뿐이며 우선 순위를 지정하는 방법이 다릅니다 첫 번째 용어 최적화에 대한 관심과 두 번째 용어 최적화에 대한 관심 그리고 당신이 원한다면 매개 변수 C가 1보다 큰 역할을한다고 생각할 수 있습니다. 그리고 그것은 두 방정식이 아니거나이 두 표현식이 동일 할 것입니다. 이것은 람다에 비해 1에 해당하며, 그렇지 않습니다. 오히려 C가 람다에 대해 1과 같으면이 두 최적화 목표는 세타에 대해 동일한 최적 값을 제공해야합니다. 그래서 여기에서 람다를 횡단하고 거기에 상수를 쓰려고합니다. .
13:37
그래서 우리는 지원 벡터 머신에 대한 전반적인 최적화 목적 함수를 제공합니다. 그리고이 함수를 최소화하면 SVM이 배운 매개 변수가 생깁니다.
13:51
마침내 로지스틱 회귀와는 달리, 지원 벡터 머신은 확률을 출력하지 않습니다. 우리가 가진 비용 함수는 우리가 매개 변수의 데이터를 얻기 위해 최소화하고, 지원 벡터 머신이하는 것입니다. y는 1 또는 0과 직접 동일합니다. 그래서 가설은
14:13
theta transpose x가 0보다 크거나 같고 그렇지 않으면 0을 예측하고 매개 변수 theta를 학습하면 지원 벡터 머신에 대한 가설의 형식입니다. 그래서 이것은 지원 벡터 머신이 무엇을하는지에 대한 수학적 정의였습니다. 다음 몇 개의 비디오에서,이 최적화 목적이 무엇에 이르게하는지, 그리고 가설 SVM의 근원을 배우게 될지에 대해 직감으로 돌아가려고합니다. 그리고 우리는 이것을 복잡한 비선형 함수에 조금 수정하는 방법에 대해서도 이야기 할 것입니다. .
