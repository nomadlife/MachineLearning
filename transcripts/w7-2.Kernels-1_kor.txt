0:00
이 비디오에서는 복잡한 비선형 분류자를 개발하기 위해 지원 벡터 머신을 적용하기 시작하고자합니다.
0:07
이를 수행하기위한 주요 기술은 커널이라고 불리는 것입니다.
0:11
이 커널이 무엇인지, 어떻게 사용하는지 보자.
0:15
이와 같이 보이는 훈련 세트가 있고 긍정적이고 부정적인 예를 구분하기 위해 비선형 결정 경계를 찾고 싶다면 어쩌면 그렇게 보이는 결정 경계 일 수 있습니다.
0:27
이렇게하는 한 가지 방법은 일련의 복잡한 다항식 기능을 제안하는 것입니다. 맞습니까? 당신이 세타 0과 플러스 세타 1 X1 더하기 점 도트 다항식 특징을 알고 있다면 1을 예측하는 가설 X로 끝나도록 0과 같은 특징 집합입니다. 그렇지 않으면.
0:51
그리고 나중에 쓰게 될 새로운 표기법을 소개하기 위해 이것을 쓰는 또 다른 방법은 이것을 사용하여 결정 경계를 계산하는 것으로 생각할 수 있다는 것입니다. 따라서 쎄타 0 플러스 쎄타 1 플러스 플러스 쎄타 2 플러스 플러스 쎄타 3 플러스 플러스 3 등등. 이 새로운 종류의 피쳐를 나타 내기 위해이 새로운 표기법 f1, f2, f3 등을 사용할 것입니다
1:19
저는 계산 중이므로 f1은 X1이고, f2는 X2와 같으며, f3는 여기에있는 것과 같습니다. 그래서, X1X2. 그래서, f4는 다음과 같습니다.
1:33
f5가 x2 제곱이되는 X1 제곱 등등 우리는 이전에 이러한 고차 다항식을 생각해내는 것이 더 많은 특징을 생각해내는 한 가지 방법이라는 것을 알았습니다.
1:45
질문은, 다른 고차원 다항식보다 더 나은 종류의 특징이 있거나, 고차 다항식이 우리가 원하는 것임을 분명히 알기 때문입니다. 입력은 픽셀이 많은 이미지입니다. 우리는 고 차수 다항식을 사용하는 것이 매우 고차원 다항식 항이 많아서 계산 상 매우 비싸게되는 것을 보았습니다.
2:11
그래서, 이런 종류의 가설 형식에 연결하기 위해 사용할 수있는 기능의 다양성이나 더 나은 선택이 있습니까? 그래서 새로운 특징 f1, f2, f3을 정의하는 방법에 대한 아이디어가 있습니다.
2:24
이 줄에서는 세 가지 새로운 기능 만 정의 할 것이지만 실제 문제에 대해서는 훨씬 더 큰 수를 정의 할 수 있습니다. 그러나 여기에 X1, X2 기능의이 단계에서해야 할 일이 있습니다. X0을 요격기 X0에서 제외하겠습니다.하지만이 단계에서는 X1 X2로,
2:42
당신은 수동으로 몇 가지 점을 선택한 다음이 점들을 l1이라고 부르며, 다른 점을 선택하려고합니다. l2를 호출하고 세 번째 점을 선택하고 이것을 l3이라고 부르겠습니다. 이 3 점을 수동으로 선택하겠습니다. 나는이 세 점을 라인 업이라고 부르겠다. 그래서 하나, 둘, 셋을 나열해라. 내가 할 일은 다음과 같이 새 기능을 정의하는 것입니다. 예제 X를 통해 첫 번째 피쳐 f1을 교육 예제 X와 첫 번째 획기적인 점과이 특정 수식의 유사성을 측정 한 것으로 정의하겠습니다. 유사성을 측정하기 위해 사용할 것이고 이것은 X의 길이에서 마이너스 l1의 제곱을 2 시그마 제곱으로 나눈 E에서 마이너스로 갈 것입니다.
3:40
따라서, 이전의 선택적인 비디오를 보았는지 여부에 따라,이 표기법은 벡터 W의 길이입니다. 그래서, 여기이 X 빼기 l1은 실제로는 단지 유클리드 거리입니다
3:58
제곱은 점 x와 표식 l1 사이의 유클리드 거리이다. 나중에 이에 대해 더 많이 알 수 있습니다.
4:06
하지만 그것은 제 첫 번째 기능이고 두 번째 기능 f2는 비슷한 X가 12와 얼마나 비슷한지를 측정하는 유사 기능이 될 것이며 게임은 다음 기능으로 정의 될 것입니다.
4:25
이것은 X와 제 2 표식 사이의 유클리드 거리의 제곱의 마이너스 값에 해당합니다. 즉 열거자를 2 시그마 제곱으로 나눈 값이며, 마찬가지로 f3은 X와 l3 사이의 유사도입니다. 다시, 유사한 공식으로.
4:46
그리고이 유사 함수가 무엇인지, 수학적 용어는 이것이 커널 함수가 될 것이라는 것입니다. 그리고 여기에서 사용하고있는 특정 커널은 실제로 가우시안 커널이라고 불립니다.
5시
그리고이 공식 때문에, 이와 같은 유사성 기능의 선택은 가우스 커널 (Gaussian kernel)이라고 불립니다. 그러나 용어가 사용되는 방식은 추상에서 이러한 서로 다른 유사 기능을 커널이라고 부르며 서로 다른 유사 기능을 가질 수 있다는 것입니다
5:13
내가 여기에서 제공하는 구체적인 예는 가우시안 커널이라고 불립니다. 우리는 다른 커널의 다른 예제를 보게 될 것이다. 그러나 지금은 이들을 유사성 함수로 생각하십시오.
5:22
그래서 X와 l 사이의 유사성을 쓰는 대신에, 우리는 이것을 여러분이 알 수있는 커널, x와 나의 랜드 마크 중 하나 사이의 소문자 k라고 쓰는 경우도 있습니다.
5:34
그래서 범인이 실제로 무엇을하는지, 그리고 이런 종류의 유사성 기능이 왜 나타나는 지, 왜 이러한 표현이 의미가 있는지 봅시다.
5:46
그래서 첫 번째 랜드 마크를 가져 가자. 나의 표식 인 l1은 내가 지금 내 모습에서 선택한 점 중 하나이다.
5:53
따라서 x와 l1 사이의 커널의 유사성은이 표현식에 의해 주어집니다.
5:57
우리가 분자 항이 무엇인지에 관해서는 같은 페이지에 있다는 것을 확실히하기 위해, 분자는 거리의 일종에 대해 J = 1에서 N까지의 합계로 쓰여질 수 있습니다. 그래서 이것은 벡터 X와 벡터 l 사이의 컴포넌트 현명한 거리입니다. 그리고이 슬라이드의 목적을 위해 다시 X0을 무시합니다. 따라서 절편 항 X0을 무시하면 항상 1입니다.
6:21
따라서 X와 랜드 마크가 비슷한 커널을 계산하는 방법입니다.
6:27
이 함수가 무엇을하는지 봅시다. X가 랜드 마크 중 하나에 가깝다고 가정합니다.
6:33
그러면이 유클리드 거리 공식과 분자는 0에 가까워 질 것입니다. 그래서, 그것은이 용어입니다. 거리는 컸고, X와 0을 사용하는 거리는 0에 가깝습니다.
6:46
f1, 이것은 간단한 기능이며 약 E가 마이너스 0이 될 것이고 그 다음에 2 이상의 제곱 된 제곱은 제곱과 같습니다.
6:55
E가 0으로, E가 0으로, E가 0으로 가깝게 될 것입니다.
7:01
거리가 정확하게 0이 아니기 때문에 근사 기호를 여기에 넣겠습니다. 그러나 X가 랜드 마크에 더 가까울 경우이 항은 0에 가까울 것이므로 f1은 1에 가까울 것입니다.
7:13
반대로, X가 01에서 멀어지면이 첫 번째 피쳐 f1은 2 제곱 제곱으로 나눈 2 제곱 제곱의 마이너스로 E가되고 큰 숫자의 마이너스로 0이 가까워 질 것입니다.
7:33
그래서 이러한 특징들은 X가 당신의 랜드 마크 중 하나에서 얼마나 유사한지를 측정하는 것이며, X가 당신의 랜드 마크에 가까울 때 피쳐 f가 1에 가깝게 될 것이며 X가 멀 때 0 또는 0에 가까워 질 것입니다 당신의 랜드 마크. 이 랜드 마크들 각각. 이전 줄에서는 l1, l2, l3이라는 3 개의 랜드 마크를 그렸습니다.
7:56
이러한 랜드 마크는 각각 새 기능 f1, f2 및 f3을 정의합니다. 즉, 트레이닝 예제 X가 주어지면 f1, f2 및 f3의 3 가지 새로운 기능을 계산할 수 있습니다. 알다시피, 지금 쓴 3 개의 랜드 마크. 그러나 먼저이 지수 함수를 살펴 보겠습니다.이 유사 함수를 살펴보고 일부 그림에서 플롯을 보자. 실제로이 모습이 더 잘 이해할 수 있습니다.
8:23
이 예에서는 X1과 X2의 두 가지 기능이 있다고 가정 해 보겠습니다. 그리고 내 첫 번째 랜드 마크, l1이 위치, 3 5에 있다고 가정 해 봅시다.
8:33
시그마 제곱이 지금과 같다고 가정 해 봅시다. 이 기능이 어떻게 보이는지 플롯하면이 수치가 나옵니다. 그래서 세로 축, 표면의 높이 값입니다
8:45
f1과 수평축의 아래쪽에는 훈련 예가 있으면 거기에 있습니다.
8:51
x1이고 x2가 있습니다. 특정 훈련 예를 들어, 여기 표면의 높이에서 x1과 x2의 값을 보여주는 훈련 예제는 f1의 상응하는 값을 보여 주며 아래의 수치는 내가 보여준 것과 동일한 수치이며, x1 수평축에 x2, 따라서 바닥에있는이 그림은 3D 표면의 등고선 플롯입니다.
9:16
X가 3 5와 정확히 같을 때 우리는 f1이 값 1을 취하고 최대 값이기 때문에 X가 멀리 떨어지면서 X가 멀리 떨어져 나가는 것을 보았습니다.이 피쳐는 값을 취합니다
9:36
0에 가깝습니다.
9:38
그리고 이것은 실제로 하나의 특징입니다. f1은 첫 번째 표식에 X가 얼마나 가깝고 X가 첫 번째 표식 l1에 얼마나 가까운 지에 따라 0과 1 사이에서 얼마나 다른지를 측정합니다.
9:52
이제 다른 하나는이 슬라이드에서이 매개 변수 σ 제곱의 변화의 효과를 보여주는 것입니다. 따라서 시그마 제곱은 가우시안 커널의 매개 변수이며이를 다양 화하면 약간 다른 효과를 얻게됩니다.
10:05
시그마 제곱을 0.5와 같게 설정하고 우리가 얻는 것을 봅시다. 시그마 스퀘어를 0.5로 설정했습니다. 범프의 너비가 좁아지는 것을 제외하고는 커널이 비슷하게 보입니다. 등고선도 조금 줄어든다. 따라서 시그마 제곱이 0.5와 같으면 X가 3 5와 같아지고 멀리 이동할 때,
10:24
피쳐 (f1)는 제로 (zero)로 훨씬 더 빠르게 반대로 떨어지고,
10:32
그 경우에 3을 얻었고, 제가 멀리 떠날 때부터 당신이 증가했으면, 당신은 알고 있습니다. 여기이 요점은 정말로, 맞습니다. l1이 위치 3 5에 있습니다. 맞습니다. 그래서 여기에 표시됩니다.
10:48
그리고 시그마 제곱이 큰 경우, 당신이 l1에서 멀어지면, 그 피쳐의 가치는 훨씬 더 천천히 떨어집니다.
11시 3 분
그래서,이 특징의 정의가 주어지면 우리가 배울 수있는 가설의 출처를 알아 봅시다.
11:09
교육 예제 X가 주어지면 이러한 기능을 계산할 것입니다.
11:14
f1, f2, f3 및 a
11:17
가설은 쎄타 0 플러스 쎄타 1 플러스 플러스 쎄타 2 f2 등을 예측하는 것입니다.이 특별한 예에서는 이미 학습 알고리즘을 발견했다고 가정 해 봅시다. 알다시피, 어떻게 든이 매개 변수 값으로 끝났습니다. 그래서 쎄타 0이 마이너스 0.5이면 쎄타 1은 1, 쎄타 2는 1, 쎄타 3은 0과 같습니다. 그리고 제가하고 싶은 것은 우리가
11:49
이 마젠타 색 점에 위치합니다. 바로 여기에이 점을 그렸습니다. 그래서 제가 훈련 예 X를 가지고 있다고 가정 해 봅시다. 제 가설은 어떻게 예측합니까? 음,이 공식을 보면.
12:04
트레이닝 예제 X가 l1에 가깝기 때문에 f1은 1에 가까울 것입니다. 트레이닝 예제 X가 l2와 I3에서 멀리 떨어져 있기 때문입니다. f2가 0에 가까울 것이고 f3가 될 것입니다. 0에 가깝습니다.
12:21
그래서, 제가 공식을 보면, 나는 쎄타 0 플러스 쎄타 1 회 1 플러스 쎄타 2 배 약간의 가치가 있습니다. 정확히 0은 아니지만 0에 가깝게 말하십시오. 그리고 0에 가깝게 3을 곱하면됩니다.
12:37
그리고 이것은 이제이 값들을 연결하는 것과 같습니다.
12:41
따라서 마이너스 0.5에 1을 곱한 1이 1이됩니다. 0.5보다 크거나 0 인 0.5와 같습니다. 따라서이 시점에서 Y는 0보다 크거나 같기 때문에 Y를 1로 예측합니다.
12:58
이제 다른 점을 생각해 봅시다. 이제는 다른 점을 말하자면,이 색을 다른 색으로 그려 보겠습니다. 청록색으로 말하자면, 거기에있는 점에 대해, 저의 훈련 예 X라면, 비슷한 계산을한다면, 그 f1, f2,
13:15
Ff3은 모두 0에 가까워 질 것입니다.
13:18
그래서 우리는 theta 0에 + 0.5가 있고 f1, f2, f3가 모두 0이기 때문에 이것은 theta 0 plus theta 1, f1, plus 등을 가지고 약 0.5가 될 것입니다. 그래서 이것은 마이너스 0.5 일 것입니다, 이것은 0보다 작습니다. 그래서,이 시점에서, Y는 Y가 0이라고 예측할 것입니다.
13:44
그리고 만약 당신이이 점을 여러 가지 다른 점에 대해서한다면, L2에 가까운 훈련 예를 가지고 있다면,이 시점에서 우리는 Y가 1이라고 예측할 것이라고 스스로 확신하게하십시오.
13:56
사실, 결국 당신이하는 일은, 당신이 경계를 돌아 보면,이 공간, 우리가 발견 할 수있는 것은 l1과 l2 근처의 점들에 대해 우리는 양의 예측을 끝내는 것입니다. 그리고 l1과 l2에서 멀리 떨어져있는 점들에 대해서, 그것은이 두 랜드 마크에서 멀리 떨어져있는 점들에 대한 것입니다. 우리는 클래스가 0과 같다고 예측하게됩니다. 결국, 우리가 끝내는 것은이 가설의 결정 경계가 이런 식으로 뭔가를 찾으십시오.이 적색 결정 경계선 안쪽에서 Y가 1을 예측하고 우리가 예측하지 못한 부분
14:32
Y는 0입니다. 그래서 이것은 랜드 마크와 커널 함수에 대한이 정의에서 어떻게 나타납니다. 우리는 두 개의 경계표 중 어느 하나에 가까울 때 우리가 긍정적 인 것을 예측 한 곳과 같이 꽤 복잡한 비선형 결정 경계를 배울 수 있습니다. 그리고 우리가 어떤 랜드 마크에서 멀리 떨어져있을 때 우리는 부정적인 예측을합니다. 그래서 이것은 커널의 개념의 일부이며 우리가 그들을 지원 벡터 머신과 함께 사용하는 방법입니다. 이는 더 복잡한 비선형 분류자를 배우기 위해 표식과 유사성 함수를 사용하여 이러한 추가 기능을 정의한다는 것입니다.
15:08
이렇게하면 커널에 대한 아이디어와이를 사용하여 지원 벡터 머신의 새로운 기능을 정의하는 방법을 알 수 있습니다.
15:15
그러나 우리가 아직 대답하지 않은 몇 가지 질문이 있습니다. 하나는 어떻게 우리가이 랜드 마크를 얻는가입니다. 우리는 어떻게 이러한 랜드 마크를 선택합니까? 그리고 또 다른 유사점은 우리가 이야기 한 것 이외에 다른 유사한 기능이 있다면 Gaussian 커널이라고 할 수 있습니다. 다음 비디오에서는 이러한 질문에 대한 답을 제공하고 커널과 함께 벡터 머신이 복잡한 비선형 함수를 학습하는 강력한 방법이 될 수있는 방법을 보여주기 위해 모든 것을 결합했습니다.
