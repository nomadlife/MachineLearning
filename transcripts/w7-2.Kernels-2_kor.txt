0:00
마지막 비디오에서 우리는 커널 아이디어와 그것이 어떻게 지원 벡터 머신을위한 새로운 기능을 정의하는데 사용될 수 있는지에 대해 이야기하기 시작했습니다. 이 비디오에서는 누락 된 세부 사항을 던지기를 원하며, 실제로 이러한 아이디어를 사용하는 방법에 대해 몇 마디 말하고 싶습니다. 예를 들어 지원 벡터 시스템에서 바이어스 분산 트레이드 오프와 관련이있는 경우와 같은 경우.
0:22
마지막 비디오에서 몇 가지 랜드 마크를 선택하는 과정에 대해 이야기했습니다. 여러분도 알다시피, l1, l2, l3 그리고 이것으로 우리는 커널이라고하는 유사성 함수를 정의 할 수있었습니다.이 유사 함수가 있다면 이것은 가우시안 커널입니다.
0:38
그리고 그것은 우리로 하여금 이런 형태의 가설 기능을 구축하게했습니다.
0:43
그러나 우리는이 랜드 마크를 어디에서 얻을 수 있습니까? 우리는 어디에서 l1, l2, l3을 얻을 수 있습니까? 또한 복잡한 학습 문제의 경우 우리가 손으로 선택할 수있는 것 중 3 가지보다 훨씬 많은 랜드 마크를 원할 수도 있습니다.
0:55
그래서 실제적으로 이것은 기계 학습 문제가 주어진 경계표가 선택되는 방법입니다. 우리는 몇 가지 긍정적 인면과 부정적인면의 데이터를 가지고 있습니다. 여기 예제가 있습니다. 우리가 가지고있는 모든 교육 예제에 대해서만 호출 할 것입니다. 랜드 마크를 교육 예제와 정확히 같은 위치에 놓을 것입니다.
1:18
따라서 x1 인 경우 하나의 훈련 예가 있으면 첫 번째 학습 예제와 동일한 위치에 xactly 위치하는 첫 번째 획기적인 점을 선택하겠습니다.
1:29
그리고 다른 트레이닝 예제가 있다면 x2. 두 번째 랜드 마크를 설정하려고합니다.
1:35
내 두 번째 훈련 사례의 위치입니다.
1:38
오른쪽의 그림에서 빨간색과 파란색 점을 일러스트레이션과 같이 사용했습니다.이 그림의 색상은 오른쪽 그림의 점 색상이 중요하지 않습니다.
1:47
하지만이 방법을 사용하여 끝낼 수있는 것은 l1, l2의 m 개의 랜드 마크로 끝날 것입니다.
1:54
트레이닝 예제 각각의 위치 당 위치 당 하나의 랜드 마크가있는 m 개의 트레이닝 예제가있는 경우 l (m)까지 내려갑니다. 내 기능이 근본적으로 내가 훈련 세트에서 본 것들 중 하나에 얼마나 가까운지를 측정 할 것이라고 말하는 것이기 때문에 이것은 좋은 일입니다. 그래서이 윤곽을 조금 더 구체적으로 작성하기 위해, m 개의 연습 예제를 통해, m 개의 연습 예제의 위치와 정확히 일치하도록 내 랜드 마크의 위치를 ​​선택하겠습니다.
2:25
예제 x가 주어 졌을 때,이 예제에서 x는 트레이닝 집합의 어떤 것일 수 있습니다. 교차 유효성 검사 집합에있는 것이거나 테스트 집합에있는 것일 수 있습니다. 예 x를 가정하면 f1, f2 등과 같이 이러한 기능을 계산할 것입니다. 여기서 l1은 실제로 x1과 같습니다. 그리고 이것들은 나에게 특징 벡터를줍니다. 그래서 f를 특징 벡터로 씁니다. 나는이 f1, f2 등을 취하여 특징 벡터로 그룹화 할 것입니다.
2:56
fm으로 가져 가라.
2:59
그리고, 당신도 알다시피, 관습에 따라. 우리가 원한다면 우리는 항상 1과 같은 여분의 피쳐 f0을 추가 할 수 있습니다. 따라서 이것은 이전에했던 것과 비슷한 역할을합니다. 우리 인터셉터 인 x0의 경우.
3:13
예를 들어, 훈련 예 x (i), y (i),
3:18
이 훈련 예제에서 우리가 계산할 기능은 다음과 같습니다 : x (i)가 주어지면 f1 (i)에 매핑합니다.
3:27
유사점은 무엇입니까? 나는 전체 단어를 쓰는 대신 SIM으로 축약 할 것입니다.
3:35
유사성, 맞죠?
3:37
그리고 f2 (i)는 x (i)와 l2 사이의 유사성과 동일하며, fm (i)는 l2
3:49
x (i)와 l (m) 사이의 유사성.
3:55
그리고 중간 어딘가에. 이 목록의 어딘가에, i 번째 구성 요소에서 실제로 f 개의 첨자 i (i) 인 하나의 피쳐 구성 요소를 갖게됩니다.이 구성 요소는 유사성이 될 것입니다
4:13
x와 l (i) 사이.
4:15
여기서 l (i)는 x (i)와 동일하므로 fi (i)는 x와 그 자체 사이의 유사점이 될 것입니다.
4:23
가우시안 커널을 사용하고 있다면 실제로 이것은 2 시그마 제곱을 기준으로 한 마이너스 0입니다. 따라서이 값은 1과 같을 것이고 괜찮습니다. 그래서이 훈련 예를위한 나의 특징 중 하나는 1과 같을 것입니다.
4:34
그리고 제가 위에있는 것과 비슷합니다. 나는 이러한 모든 특징들을 취하여 그것들을 특징 벡터로 묶을 수있다. 따라서 저는 R (n)과 R (n)의 차원 벡터를 사용하여 x (i)를 사용하여 예제를 표현하는 대신에 x (i)를 사용합니다.
4:48
용어를 설정할 수 있는지 여부에 따라 R (n) 또는 R (n)에 1을 더한 것입니다. 대신이 특성 벡터 f를 사용하여 연습 예제를 나타낼 수 있습니다. 나는이 위 첨자를 쓸 것이다. 어느 것이이 모든 것들을 가져 와서 벡터에 쌓을 것입니다. 그래서, f1 (i)부터 fm (i)까지 내려야합니다. 그리고 원한다면 보통 f0 (i)를 추가 할 것입니다. f0 (i)는 1과 같습니다. 내 훈련 예를 나타내는 특징 벡터. 이 커널과 유사 함수가 주어지면 여기에 간단한 벡터 머신을 사용하는 방법이 있습니다. 이미 매개 변수 세타의 학습 세트가있는 경우 x 값을 지정하고 예측을하고 싶습니다.
5:41
이제는 R (m) + 1 차원 특징 벡터 인 피처 f를 계산합니다.
5:49
그리고 우리는 m 개의 훈련 예제와 m 개의 랜드 마크를 가지고 있기 때문에 여기에 있습니다. 우리가하는 일은 theta transpose f가 0보다 크거나 같으면 1을 예측하는 것입니다. 맞습니다. 따라서 theta transpose f가 물론, theta 0, f 0 plus theta 1, f 1 plus dot dot dot, theta m f (m)와 동일합니다. 그리고 저는 제 파라메터 벡터 인 theta가 또한 m plus 1 차원 벡터가 될 것입니다. 랜드 마크의 수와 트레이닝 세트의 크기가 같기 때문에 여기에 m이 있습니다. 그래서 m은 훈련 세트 크기 였고, 이제는 매개 변수 벡터가 m 플러스 1 차원이 될 것입니다.
6:32
따라서 매개 변수의 세타에 대한 설정이 이미있는 경우 예측을 수행하는 방법입니다. 매개 변수의 세타를 어떻게 구합니까? SVM 학습 알고리즘을 사용하면 그렇게 할 수 있습니다. 특히이 최소화 문제를 해결하면됩니다. 우리는 전에 가지고 있던이 비용 함수의 C의 매개 변수의 세타를 최소화했습니다. 지금은 원래의 특징 인 x (i)를 사용하여 theta transpose x (i)를 사용하여 예측을하는 대신 거기를 보지 않고 대신 보게됩니다. 대신에 우리는 x (i)의 특징을 가져 와서 새로운 기능으로 대체했습니다.
7:07
그래서 우리는 훈련 예제에 대한 예측을하기 위해 theta transpose f (i)를 사용하고 있습니다. 그리고 여기 아시다시피 두 곳 모두에서이 최소화 문제를 해결함으로써 Support Vector Machine에 대한 매개 변수를 얻습니다. .
7:23
마지막으로 하나의 세부 사항은 우리가 실제로 가지고있는 최적화 문제가 m 개의 피쳐와 같기 때문입니다. 그것이 여기 있습니다. 우리가 가지고있는 기능의 수.
7:37
실제로, 우리가 가지고있는 유효 피처 수는 f의 차원입니다. 그래서 n은 실제로 m과 같을 것입니다. 여러분이 원한다면 이것을 합계라고 생각할 수 있습니다. 이것은 정말로 j가 1부터 m까지 합계입니다. 그리고 이것을 생각할 수있는 한 가지 방법은 f가 새로운 특징이 아니라면, 우리는 m plus 1 특징을 가지고 있고, 요격기에서 나오는 더하기 1을 가지고 있기 때문에 이것을 n과 같다고 생각할 수 있습니다.
8:05
그리고 여기에서 우리는 정규화에 대한 우리의 이전 비디오와 유사하게 여전히 매개 변수 theta를 정규화하지 않기 때문에 j가 1부터 n까지 합계를 계산합니다. 이것이 j가 j 대신에 1에서 m까지의 합인 이유입니다 m은 0이다. 그래서 지원 벡터 머신 학습 알고리즘입니다. 그것은 제가 언급해야 할 수학적 세부 사항을 제외하고는 지원 벡터 머신이 구현되는 방식입니다.이 마지막 용어는 실제로 조금씩 다르게 수행됩니다. 따라서 지원 벡터 머신을 사용하기 위해서는이 마지막 세부 사항을 알 필요가 없습니다. 사실 여기에 적혀있는 방정식은 필요한 모든 직관을 제공해야합니다. 그러나 지원 벡터 머신이 구현되는 방식에서 알다시피, 그 용어는 theta j 제곱의 j의 합계입니까?
8:53
이것을 쓰는 또 다른 방법은 우리가 매개 변수 theta 0을 무시하면 theta transpose theta로 쓰여질 수 있습니다. 따라서 theta 1은 theta m까지 내려갑니다. 쎄타 0 무시.
9:11
그런 다음이 합계 j of theta j j는 이것이 또한 theta transpose theta로 쓰여질 수 있다는 것을 제곱합니다.
9:19
그리고 대부분의 벡터 머신 구현을 지원하는 것은 실제로이 theta transpose theta를 대체합니다. 대신에 theta transpose는 커널을 사용하는 시간, theta에 따라 달라집니다. 그래서 이것은 우리에게 약간 다른 거리 측정 기준을 제공합니다. 정확하게 최소화하는 대신 약간 다른 방법을 사용합니다.
9:41
세타 제곱의 규범은 그것과 약간 비슷한 것을 최소화하는 것을 의미합니다. 이것은 커널에 의존하는 매개 변수 벡터의 재조정 버전과 같습니다. 그러나 이것은 수학적인 종류의 일종입니다. 이를 통해 지원 벡터 머신 소프트웨어가 훨씬 효율적으로 실행될 수 있습니다.
9:58
그리고 지원 벡터 머신이이 작업을 수행하는 이유는이 수정 사항 때문입니다. 훨씬 더 큰 훈련 세트로 확장 할 수 있습니다. 예를 들어 10,000 개의 교육 사례가있는 교육 세트가있는 경우입니다.
10:12
그런 다음, 우리가 랜드 마크를 정의하는 방식으로, 10,000 개의 랜드 마크로 끝납니다.
10:16
그래서 세타는 10,000 차원이됩니다. 어쩌면 그렇게 할 수 있습니다. 그러나 m이 정말로 커지고 이러한 매개 변수를 모두 해결하면 m이 50,000 또는 100,000이면 이러한 매개 변수를 모두 풀면 지원 벡터 머신 최적화 소프트웨어에 비용이 많이들 수 있습니다. 내가 여기에서 그린 최소화 문제를 풀었다. 수학적 세부 사항으로, 다시는 알 필요가 없습니다.
10:41
실제로 마지막 term을 약간 변형시켜 단지 theta의 theta 제곱의 norm 제곱을 최소화하는 것과 약간 다른 것을 최적화합니다. 하지만 원한다면, 객관적으로 약간 변경하는 일종의 구현 세부 사항이라고 자유롭게 생각할 수 있지만 주로 계산 효율성의 이유로 수행됩니다. 따라서 대개이 문제에 대해 걱정할 필요가 없습니다. .
11:07
그런데 로지스틱 회귀와 같은 다른 알고리즘에도 커널의 아이디어를 적용하지 않는 이유에 대해 궁금한 점이 있다면 원하는 경우 실제로 커널의 아이디어를 적용하고 랜드 마크를 사용하여 기능의 소스를 정의 할 수 있습니다. 로지스틱 회귀 (logistic regression) 등등. 그러나 지원 벡터 머신에 적용되는 계산 트릭은 로지스틱 회귀와 같은 다른 알고리즘으로 일반화되지 않습니다. 그래서 로지스틱 회귀 분석을하는 커널을 사용하는 것은 너무 느리게 진행되는 반면, 구현 된 기법과이를 수정하는 방법과 지원 벡터 머신 소프트웨어가 어떻게 구현되는지에 대한 세부 사항 때문에 지원 벡터 머신과 커널은 특히 그러하다. 잘 함께. 반면, 로지스틱 회귀와 커널은 할 수 있지만 매우 느리게 실행됩니다. 또한 커널이있는 지원 벡터 시스템을 실행하는 특별한 경우에 대해 사람들이 알아 낸 고급 최적화 기술을 활용할 수 없습니다. 하지만이 모든 것은 비용 기능을 최소화하기 위해 실제로 소프트웨어를 구현하는 방법에만 적용됩니다. 다음 비디오에서 그 점에 대해 더 이야기 할 것이지만이 비용 기능을 최소화하기 위해 소프트웨어를 작성하는 방법에 대해 알 필요가 없습니다. 그렇게하기 위해 선반 소프트웨어를 매우 잘 찾을 수 있기 때문입니다.
12:18
그리고 행렬을 뒤집거나 제곱근을 계산하는 코드를 작성하는 것은 권장하지 않습니다. 실제로이 비용 함수를 최소화하기 위해 소프트웨어를 작성하는 것은 권장하지 않지만 선반 소프트웨어 패키지를 사용하는 것이 좋습니다. 이러한 소프트웨어 패키지는 이미 이러한 수치 최적화 기법을 구현하고 있으므로,
12:39
그래서 당신은 정말로 그들에 대해 걱정할 필요가 없습니다. 그러나 알고 있어야 할 또 하나의 사실은 지원 벡터 머신을 적용 할 때 지원 벡터 머신의 매개 변수를 어떻게 선택할까요?
12:51
그리고이 비디오에서 마지막으로하고 싶은 것은 지원 벡터 머신을 사용할 때의 바이어스 및 분산 트레이드 오프에 대한 약간의 말입니다. SVM을 사용할 때, 당신이 선택해야 할 것들 중 하나는 최적화 목적에있는 매개 변수 C이고, C는 1과 비슷한 역할을했던 것을 기억합니다. 여기서 λ는 우리가 로지스틱 회귀에 대해 갖는 정규화 매개 변수입니다.
13:15
그래서 여러분이 C의 값이 크다면, 이것은 많은 정규화를 사용하지 않는 람다 의미의 작은 가치에 대해 우리가 로지스틱 회귀에서 되돌아 왔던 것과 같습니다. 그리고 그렇게하면 편견이 낮고 분산이 큰 가설을 세우는 경향이 있습니다.
13:30
작은 값의 C를 사용하는 경우, 람다의 값이 큰 로지스틱 회귀를 사용하는 경우에 해당하며, 편향이 높고 분산이 낮은 가설에 해당합니다. 그래서 큰 C를 가진 가설은 더 높은 분산을 가지며 과도하게 적용되기 쉽다. 반면 작은 C를 가진 가설은 더 높은 편견을 가지므로 덜 적합하다.
13:56
따라서이 매개 변수 C는 우리가 선택해야하는 매개 변수 중 하나입니다. 다른 하나는 매개 변수입니다.
14:02
시그마 제곱은 가우시안 커널에 나타납니다.
14:05
따라서 Gaussian kernel 시그마 제곱이 크면 유사 함수에서 E가 마이너스 x에서 랜드 마크까지를 알 수 있습니다.
14:16
2 시그마 제곱에 대해 제곱에 비례합니다.
14:20
이 예제 중 하나에서; 하나의 피쳐, x1이있는 경우, 그 위치에 랜드 마크가 있으면 시그마 제곱이 클 경우 가우스 커널이 상대적으로 느리게 떨어지는 경향이 있습니다
14:33
그래서 이것은 내 특징 f (i)이 될 것이므로 부드럽게 변화하는보다 부드러운 함수가 될 것이므로 가우스 커널이 부드럽게 떨어지면 더 높은 편견과 낮은 분산을 갖는 가설을 얻을 수 있습니다. 천천히 변하는 가설을 세우거나 입력 x를 변경하면 매끄럽게 변화합니다. 반대로 시그마 제곱이 작 으면 제 1 특징 x1을 부여한 제 랜드 마크입니다. 내 유사성 기능 인 나의 가우스 커널은 갑자기 다양합니다. 그리고 두 경우 모두 1을 골랐습니다. 따라서 시그마 제곱이 작 으면 내 기능이 덜 매끄럽게 변합니다. 그래서 여기가 더 높은 경사면이거나 더 높은 파생물 일 겁니다. 그리고 이것을 사용하면 더 낮은 바이어스에 대한 가설을 끝내고 더 높은 분산을 가질 수 있습니다.
15:23
이번 주 요점을 살펴보면 실제로 이러한 아이디어 중 일부를 가지고 놀고 직접 이러한 효과를 볼 수 있습니다.
15:31
그래서, 그것은 커널 알고리즘을 가진 지원 벡터 머신이었습니다. 바라건대 편향과 분산에 대한이 토론은이 알고리즘이 어떻게 동작하는지 기대할 수있는 방법을 제공 할 것입니다.
